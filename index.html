<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>GradualWanda UI</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 20px;
    }
    /* 讓「1. 設定模型路徑」與「2. 功能勾選區」並排 */
    .top-row {
      display: flex;           /* 讓子元素在一行排列 */
      gap: 20px;              /* 區塊之間留點空隙 */
      align-items: flex-start; /* 對齊到上方(可改為center等) */
    }
    /* 其他區塊的樣式維持不變 */
    .config-container {
      margin: 0.5rem 0 1rem 0;
      padding: 0.5rem;
      border: 1px solid #ccc;
    }
    .config-container label {
      display: inline-block;
      width: 200px;
      margin-right: 10px;
      margin-top: 5px;
    }
    .section-title {
      margin-top: 1rem;
      margin-bottom: 0.5rem;
      font-weight: bold;
      font-size: 1.1em;
    }
  </style>
</head>
<body>
  <h1>GradualWanda UI</h1>
  
  <!-- (A) 「1. 設定模型路徑」與「2. 選擇要執行的功能」並排 -->
  <div class="top-row">
    <!-- (A1) 模型路徑設定 -->
    <div>
      <h2>1. 設定模型路徑</h2>
      <input type="text" id="modelPath" placeholder="輸入模型路徑" />
      <button onclick="setModelPath()">設定</button>
      <p id="modelPathResult"></p>
    </div>

    <!-- (A2) 功能勾選區 -->
    <div>
      <h2>2. 選擇要執行的功能</h2>
      <label>
        <input type="checkbox" id="checkEvaluate" />
        Evaluate
      </label><br>

      <label>
        <input type="checkbox" id="checkGradualPruning" />
        Gradual Pruning
      </label><br>

      <label>
        <input type="checkbox" id="checkLoRAFinetune" />
        LoRA Finetune
      </label><br>

      <label>
        <input type="checkbox" id="checkPruneWanda" />
        Prune Wanda
      </label><br>

      <label>
        <input type="checkbox" id="checkEvaluateSparsity" />
        Evaluate Sparsity
      </label><br><br>
    </div>
  </div>

  <!-- (C) EvaluateConfig 表單 -->
  <div class="config-container">
    <div class="section-title">EvaluateConfig 參數</div>
    <div>
      <label for="eval_ntrain">ntrain:</label>
      <input type="number" id="eval_ntrain" value="5" />
    </div>
    <div>
      <label for="eval_data_dir">data_dir:</label>
      <input type="text" id="eval_data_dir" value="data" />
    </div>
    <div>
      <label for="eval_save_dir">save_dir:</label>
      <input type="text" id="eval_save_dir" value="output" />
    </div>
    <div>
      <label for="eval_engine">engine (逗號分隔):</label>
      <input type="text" id="eval_engine" value="llama2" />
    </div>
  </div>

  <!-- (D) GradualConfig 表單 -->
  <div class="config-container">
    <div class="section-title">GradualConfig 參數</div>
    <div>
      <label for="grad_model_name">model_name:</label>
      <input type="text" id="grad_model_name" value="llama2-7b" />
    </div>
    <div>
      <label for="grad_total_steps">total_steps:</label>
      <input type="number" id="grad_total_steps" value="5" />
    </div>
    <div>
      <label for="grad_final_sparsity">final_sparsity:</label>
      <input type="number" step="0.01" id="grad_final_sparsity" value="0.8" />
    </div>
    <div>
      <label for="grad_nsamples">nsamples:</label>
      <input type="number" id="grad_nsamples" value="2" />
    </div>
    <div>
      <label for="grad_cache_dir">cache_dir:</label>
      <input type="text" id="grad_cache_dir" value="llm_weights" />
    </div>
  </div>

  <!-- (E) LoRaConfig 表單 -->
  <div class="config-container">
    <div class="section-title">LoRaConfig 參數</div>
    <div>
      <label for="lora_r">r:</label>
      <input type="number" id="lora_r" value="8" />
    </div>
    <div>
      <label for="lora_alpha">lora_alpha:</label>
      <input type="number" id="lora_alpha" value="32" />
    </div>
    <div>
      <label for="lora_target_modules">target_modules (逗號):</label>
      <input type="text" id="lora_target_modules" value="q_proj,v_proj" />
    </div>
    <div>
      <label for="lora_dropout">lora_dropout:</label>
      <input type="number" step="0.01" id="lora_dropout" value="0.1" />
    </div>
    <div>
      <label for="lora_bias">bias:</label>
      <input type="text" id="lora_bias" value="none" />
    </div>
    <div>
      <label for="lora_task_type">task_type:</label>
      <input type="text" id="lora_task_type" value="CAUSAL_LM" />
    </div>
    <div>
      <label for="lora_epochs">epochs:</label>
      <input type="number" id="lora_epochs" value="2" />
    </div>
    <div>
      <label for="lora_batch_size">per_device_train_batch_size:</label>
      <input type="number" id="lora_batch_size" value="1" />
    </div>
    <div>
      <label for="lora_max_length">max_length:</label>
      <input type="number" id="lora_max_length" value="256" />
    </div>
    <div>
      <label for="lora_output_dir">output_dir:</label>
      <input type="text" id="lora_output_dir" value="lora_finetuned_model" />
    </div>
  </div>

  <!-- (F) PruningConfig (Wanda) 表單 -->
  <div class="config-container">
    <div class="section-title">PruningConfig 參數</div>
    <div>
      <label for="prune_seed">seed:</label>
      <input type="number" id="prune_seed" value="0" />
    </div>
    <div>
      <label for="prune_nsamples">nsamples:</label>
      <input type="number" id="prune_nsamples" value="2" />
    </div>
    <div>
      <label for="prune_sparsity_ratio">sparsity_ratio:</label>
      <input type="number" step="0.01" id="prune_sparsity_ratio" value="0.0" />
    </div>
    <div>
      <label for="prune_sparsity_type">sparsity_type:</label>
      <input type="text" id="prune_sparsity_type" value="unstructured" />
    </div>
    <div>
      <label for="prune_cache_dir">cache_dir:</label>
      <input type="text" id="prune_cache_dir" value="llm_weights" />
    </div>
    <div>
      <label for="prune_use_variant">use_variant:</label>
      <input type="checkbox" id="prune_use_variant" />
    </div>
    <div>
      <label for="prune_save">save:</label>
      <input type="text" id="prune_save" placeholder="可留空" />
    </div>
    <div>
      <label for="prune_save_model">save_model:</label>
      <input type="text" id="prune_save_model" placeholder="可留空" />
    </div>
  </div>

  <!-- (G) 執行按鈕 & 結果區 -->
  <div>
    <button onclick="executeSelectedFunctions()">執行已勾選功能</button>
    <p id="apiResult"></p>
  </div>

  <script>
    /************************************************************
     * (A) 設定模型路徑
     ************************************************************/
    async function setModelPath() {
      const path = document.getElementById('modelPath').value;
      try {
        const response = await fetch('/api/set_model_path', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ modelPath: path })
        });
        const data = await response.json();
        document.getElementById('modelPathResult').innerText = data.message;
      } catch (err) {
        console.error(err);
        document.getElementById('modelPathResult').innerText = '發生錯誤';
      }
    }

    /************************************************************
     * (G) 執行勾選功能 → 收集表單欄位並呼叫後端 API
     ************************************************************/
    async function executeSelectedFunctions() {
      const resultElement = document.getElementById('apiResult');
      resultElement.innerText = ''; // 執行前先清空

      let messages = [];

      try {
        // 1) Evaluate
        if (document.getElementById('checkEvaluate').checked) {
          const evaluateConfig = {
            ntrain: parseInt(document.getElementById('eval_ntrain').value || "5"),
            data_dir: document.getElementById('eval_data_dir').value || "data",
            save_dir: document.getElementById('eval_save_dir').value || "output",
            engine: (document.getElementById('eval_engine').value || "llama2")
                      .split(",").map(e => e.trim()),
          };

          let res = await fetch('/api/evaluate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ evaluateConfig })
          });
          let data = await res.json();
          messages.push('Evaluate: ' + data.result);
        }

        // 2) Gradual Pruning
        if (document.getElementById('checkGradualPruning').checked) {
          const gradualConfig = {
            model_name: document.getElementById('grad_model_name').value || "llama2-7b",
            total_steps: parseInt(document.getElementById('grad_total_steps').value || "5"),
            final_sparsity: parseFloat(document.getElementById('grad_final_sparsity').value || "0.8"),
            nsamples: parseInt(document.getElementById('grad_nsamples').value || "2"),
            cache_dir: document.getElementById('grad_cache_dir').value || "llm_weights",
          };

          let res = await fetch('/api/gradual_pruning', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ gradualConfig })
          });
          let data = await res.json();
          messages.push('Gradual Pruning: ' + data.result);
        }

        // 3) LoRA Finetune
        if (document.getElementById('checkLoRAFinetune').checked) {
          const loraConfig = {
            r: parseInt(document.getElementById('lora_r').value || "8"),
            lora_alpha: parseInt(document.getElementById('lora_alpha').value || "32"),
            target_modules: (document.getElementById('lora_target_modules').value || "q_proj,v_proj")
                             .split(",").map(x => x.trim()),
            lora_dropout: parseFloat(document.getElementById('lora_dropout').value || "0.1"),
            bias: document.getElementById('lora_bias').value || "none",
            task_type: document.getElementById('lora_task_type').value || "CAUSAL_LM",
            epochs: parseInt(document.getElementById('lora_epochs').value || "2"),
            per_device_train_batch_size: parseInt(document.getElementById('lora_batch_size').value || "1"),
            max_length: parseInt(document.getElementById('lora_max_length').value || "256"),
            output_dir: document.getElementById('lora_output_dir').value || "lora_finetuned_model",
          };

          let res = await fetch('/api/lora_finetune', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ loraConfig })
          });
          let data = await res.json();
          messages.push('LoRA Finetune: ' + data.result);
        }

        // 4) Prune Wanda
        if (document.getElementById('checkPruneWanda').checked) {
          const pruningConfig = {
            seed: parseInt(document.getElementById('prune_seed').value || "0"),
            nsamples: parseInt(document.getElementById('prune_nsamples').value || "2"),
            sparsity_ratio: parseFloat(document.getElementById('prune_sparsity_ratio').value || "0.0"),
            sparsity_type: document.getElementById('prune_sparsity_type').value || "unstructured",
            cache_dir: document.getElementById('prune_cache_dir').value || "llm_weights",
            use_variant: document.getElementById('prune_use_variant').checked,
            save: document.getElementById('prune_save').value || null,
            save_model: document.getElementById('prune_save_model').value || null,
          };

          let res = await fetch('/api/prune_wanda', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ pruningConfig })
          });
          let data = await res.json();
          messages.push('Prune Wanda: ' + data.result);
        }

        // 5) Evaluate Sparsity (GET, 假設無需參數)
        if (document.getElementById('checkEvaluateSparsity').checked) {
          let res = await fetch('/api/evaluate_model_sparsity');
          let data = await res.json();
          messages.push('Evaluate Sparsity: ' + data.result);
        }

        // 顯示執行結果
        resultElement.innerText = messages.join('\n');
      } catch (err) {
        console.error(err);
        resultElement.innerText = '執行過程發生錯誤：' + err;
      }
    }
  </script>
</body>
</html>
